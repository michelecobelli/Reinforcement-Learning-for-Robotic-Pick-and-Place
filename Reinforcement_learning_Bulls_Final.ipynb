{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LxLSCV0MPGTD",
        "jVnmUtpYQg53",
        "IEX_JTMYh6Dt",
        "bkPYtf4KiTDG",
        "9jMm37sxRBAe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Project Overview\n",
        "This project explores how a robot can learn to **navigate a 2D space** with obstacles of varying severity using **reinforcement learning (RL)**. Inspired by real-world robotic applications such as **pick-and-place tasks**, warehouse logistics, and mobile navigation (e.g., rovers), we simulate how an agent makes intelligent movement decisions in a grid-based environment.\n",
        "\n",
        "We use a custom **Gymnasium** environment where:\n",
        "- The agent must reach a goal zone from a random starting point.\n",
        "- The grid contains obstacles of different risk levels:\n",
        "  - üîµ Low-risk (can pass if necessary)\n",
        "  - üü® Medium-risk (to be avoided if possible)\n",
        "  - üî¥ High-risk (must avoid completely)\n",
        "- The environment randomizes each episode to promote generalization.\n",
        "\n",
        "## üß† RL Strategy\n",
        "The agent is trained using **Q-learning**, a classic value-based reinforcement learning algorithm. Reward shaping encourages it to:\n",
        "- Move toward the goal\n",
        "- Avoid dangerous areas\n",
        "- Learn efficient, safe paths\n",
        "\n",
        "Over multiple episodes, the agent develops a **policy** to navigate intelligently ‚Äî even in previously unseen environments.\n",
        "\n",
        "## ü¶æ Relevance to Robotics\n",
        "This simulation reflects the early stages of an **autonomous robotic system**, where a computer vision pipeline would first map the space and classify obstacles. The RL model would then:\n",
        "- Interpret this mapped environment as a grid\n",
        "- Learn to navigate based on obstacle types and positions\n",
        "- Inform **real-time motion planning** for robotic arms or mobile platforms\n",
        "\n",
        "This is particularly useful for:\n",
        "- Pick-and-place in dynamic fabrication settings\n",
        "- Indoor warehouse navigation\n",
        "- Autonomous rover path planning in unstructured environments"
      ],
      "metadata": {
        "id": "7fcZo2BAPOGL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üõ†Ô∏è Install Dependencies (Colab)\n",
        "This installs required tools to run Pygame, OpenGL, and video capture in a Colab notebook.\n"
      ],
      "metadata": {
        "id": "LxLSCV0MPGTD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2RP5i5XayWz",
        "outputId": "53dd9dd3-c5cf-4c8d-ae86-18f82d660d0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3-opengl is already the newest version (3.1.5+dfsg-1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.13).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.11/dist-packages (3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium pygame numpy matplotlib tqdm\n",
        "!apt-get install -y xvfb python3-opengl\n",
        "!pip install pyvirtualdisplay opencv-python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üñ•Ô∏è Start Virtual Display\n",
        "This allows Pygame to render graphics in a hidden display in Colab.\n"
      ],
      "metadata": {
        "id": "jVnmUtpYQg53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Start a virtual display\n",
        "display = Display(visible=0, size=(600, 600))\n",
        "display.start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFi9jj7-a7-C",
        "outputId": "f9cda897-f5b8-4ee9-d8a6-c02cc6959da5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7c3ed1822bd0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåç Custom GridWorld Environment\n",
        "This environment is built using Gymnasium and Pygame to simulate a 15x15 grid world with:\n",
        "- 8-directional movement\n",
        "- Random single-cell obstacles (3 severity levels)\n",
        "- Goal in the middle of the last row\n",
        "- Custom reward shaping\n",
        "- Visual rendering using Pygame\n"
      ],
      "metadata": {
        "id": "37DClrJ_QqSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import pygame\n",
        "import random\n",
        "\n",
        "class GridWorldEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 10}\n",
        "\n",
        "    def __init__(self, grid_size=15, max_steps=100, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.grid_size = grid_size\n",
        "        self.max_steps = max_steps\n",
        "        self.render_mode = render_mode\n",
        "        self.step_count = 0\n",
        "\n",
        "        self.observation_space = spaces.Box(low=0, high=9, shape=(grid_size, grid_size), dtype=np.uint8)\n",
        "        self.action_space = spaces.Discrete(8)\n",
        "\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_cells = [(grid_size - 1, 4), (grid_size - 1, 5)]\n",
        "        self.grid = np.zeros((grid_size, grid_size), dtype=np.uint8)\n",
        "\n",
        "        pygame.init()\n",
        "        self.cell_size = 50\n",
        "        self.window_size = self.grid_size * self.cell_size\n",
        "        self.screen = pygame.display.set_mode((self.window_size, self.window_size))\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self.grid.fill(0)\n",
        "        self.step_count = 0\n",
        "        self.agent_pos = [0, random.randint(0, self.grid_size - 1)]\n",
        "        self._place_obstacles()\n",
        "        self._update_grid()\n",
        "        return self.grid.copy(), {}\n",
        "\n",
        "    def _place_obstacles(self, count=15):\n",
        "        for _ in range(count):\n",
        "            x, y = random.randint(1, self.grid_size - 2), random.randint(0, self.grid_size - 1)\n",
        "            if (x, y) not in self.goal_cells:\n",
        "                severity = np.random.choice([1, 2, 3], p=[0.5, 0.3, 0.2])\n",
        "                self.grid[x, y] = severity\n",
        "\n",
        "    def _update_grid(self):\n",
        "        self.grid[self.grid == 8] = 0\n",
        "        for gx, gy in self.goal_cells:\n",
        "            self.grid[gx, gy] = 9\n",
        "        self.grid[tuple(self.agent_pos)] = 8\n",
        "\n",
        "    def _distance_to_closest_goal(self, x, y):\n",
        "        return min(np.sqrt((gx - x)**2 + (gy - y)**2) for gx, gy in self.goal_cells)\n",
        "\n",
        "    def _is_valid_move(self, nx, ny):\n",
        "        if 0 <= nx < self.grid_size and 0 <= ny < self.grid_size:\n",
        "            target_cell = int(self.grid[nx, ny])\n",
        "            return target_cell not in [2, 3]  # avoid yellow and red\n",
        "        return False\n",
        "\n",
        "    def step(self, action):\n",
        "        self.step_count += 1\n",
        "        x, y = self.agent_pos\n",
        "        reward = -0.1\n",
        "        done = False\n",
        "\n",
        "        moves = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "        attempted = set()\n",
        "\n",
        "        while True:\n",
        "            dx, dy = moves[action]\n",
        "            nx, ny = x + dx, y + dy\n",
        "\n",
        "            if self._is_valid_move(nx, ny):\n",
        "                old_dist = self._distance_to_closest_goal(x, y)\n",
        "                new_dist = self._distance_to_closest_goal(nx, ny)\n",
        "\n",
        "                if new_dist < old_dist:\n",
        "                    reward += 1\n",
        "                elif new_dist > old_dist:\n",
        "                    reward -= 0.5\n",
        "\n",
        "                if self.grid[nx, ny] == 1:\n",
        "                    reward += -1\n",
        "\n",
        "                if (nx, ny) in self.goal_cells:\n",
        "                    reward = 100\n",
        "                    done = True\n",
        "\n",
        "                self.agent_pos = [nx, ny]\n",
        "                moved = True\n",
        "                break\n",
        "            else:\n",
        "                reward -= 2\n",
        "                attempted.add(action)\n",
        "                if len(attempted) == self.action_space.n:\n",
        "                    moved = False\n",
        "                    break\n",
        "                remaining = list(set(range(self.action_space.n)) - attempted)\n",
        "                action = random.choice(remaining)\n",
        "\n",
        "        if self.step_count >= self.max_steps:\n",
        "            done = True\n",
        "\n",
        "        self._update_grid()\n",
        "        return self.grid.copy(), reward, done, False, {\"moved\": moved}\n",
        "\n",
        "    def render(self):\n",
        "        self.screen.fill((0, 0, 0))\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "                rect = pygame.Rect(j*self.cell_size, i*self.cell_size, self.cell_size, self.cell_size)\n",
        "                color = (200, 200, 200)\n",
        "                if self.grid[i, j] == 1:\n",
        "                    color = (0, 0, 255)\n",
        "                elif self.grid[i, j] == 2:\n",
        "                    color = (255, 255, 0)\n",
        "                elif self.grid[i, j] == 3:\n",
        "                    color = (255, 0, 0)\n",
        "                elif self.grid[i, j] == 8:\n",
        "                    color = (0, 255, 0)\n",
        "                elif (i, j) in self.goal_cells:\n",
        "                    color = (0, 0, 0)\n",
        "                pygame.draw.rect(self.screen, color, rect)\n",
        "\n",
        "                if (i, j) in self.goal_cells:\n",
        "                    cx = j * self.cell_size + self.cell_size // 2\n",
        "                    cy = i * self.cell_size + self.cell_size // 2\n",
        "                    points = [(cx - 12, cy), (cx + 12, cy - 12), (cx + 12, cy + 12)]\n",
        "                    pygame.draw.polygon(self.screen, (255, 255, 255), points)\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(10)\n",
        "\n",
        "        if self.render_mode == \"rgb_array\":\n",
        "            data = pygame.surfarray.array3d(pygame.display.get_surface())\n",
        "            return np.transpose(data, (1, 0, 2))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aoY7GWCJbDNB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Train Agent using Q-Learning\n",
        "This section uses a simple Q-learning algorithm to train the agent to reach the goal efficiently.\n",
        "- Uses Œµ-greedy exploration\n",
        "- Updates Q-table only on valid moves\n",
        "- Using 20000 episodes\n",
        "- Stores total reward per episode\n",
        "\n"
      ],
      "metadata": {
        "id": "2xyNv-xlg9Y9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorldEnv(grid_size=15, max_steps=100)\n",
        "\n",
        "q_table = np.zeros((env.grid_size, env.grid_size, env.action_space.n))\n",
        "alpha = 0.1\n",
        "gamma = 0.95\n",
        "epsilon = 0.2\n",
        "episodes = 20000\n",
        "rewards_per_episode = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    pos = env.agent_pos\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        x, y = pos\n",
        "        action = np.random.choice(env.action_space.n) if np.random.rand() < epsilon else np.argmax(q_table[x, y])\n",
        "        _, reward, done, _, info = env.step(action)\n",
        "        new_x, new_y = env.agent_pos\n",
        "        if info.get(\"moved\", True):\n",
        "            q_table[x, y, action] = (1 - alpha) * q_table[x, y, action] + \\\n",
        "                alpha * (reward + gamma * np.max(q_table[new_x, new_y]))\n",
        "            pos = [new_x, new_y]\n",
        "        total_reward += reward\n",
        "\n",
        "    rewards_per_episode.append(total_reward)\n"
      ],
      "metadata": {
        "id": "_LY_MuDog8s2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üé• Define Video Saving Function\n",
        "This function saves a list of frames (from the agent's run) as an MP4 video.\n"
      ],
      "metadata": {
        "id": "IEX_JTMYh6Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def save_video(frames, filename=\"output.mp4\", fps=10):\n",
        "    height, width, _ = frames[0].shape\n",
        "    writer = cv2.VideoWriter(filename, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "    for frame in frames:\n",
        "        writer.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "    writer.release()\n"
      ],
      "metadata": {
        "id": "UwbTuwBuh_RE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Evaluate Trained Agent and Record Video\n",
        "Runs the trained agent on 5 new random environments.\n",
        "Captures its movement and creates a video using the previously defined function.\n"
      ],
      "metadata": {
        "id": "bkPYtf4KiTDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = GridWorldEnv(grid_size=15, max_steps=100, render_mode=\"rgb_array\")\n",
        "\n",
        "test_frames = []\n",
        "for ep in range(10):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    pos = env.agent_pos\n",
        "\n",
        "    while not done:\n",
        "        x, y = pos\n",
        "        action = np.argmax(q_table[x, y])\n",
        "        _, _, done, _, _ = env.step(action)\n",
        "        frame = env.render()\n",
        "        if frame is not None:\n",
        "            test_frames.append(frame)\n",
        "        pos = env.agent_pos\n",
        "\n",
        "save_video(test_frames, \"trained_agent_video.mp4\", fps=7)\n",
        "print(\"‚úÖ Video saved as trained_agent_video.mp4\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vi11jBmPiRVP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e0beab6-f1e4-4d98-93c5-e226dad7d389"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Video saved as trained_agent_video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üíæ Render and Save 3D Video (10 Episodes)\n",
        "\n",
        "Using the captured frames and positions, this block renders a fully 3D animation of the agent navigating through 10 different randomized environments.\n",
        "\n",
        "The video is saved as:\n",
        "\n",
        "üìÅ `agent_3d_path_10episodes.mp4`\n",
        "\n",
        "Each frame is properly scaled using:\n",
        "- Grid cells = `3x3` units\n",
        "- Accurate obstacle heights\n",
        "- Transparent obstacles for clear agent visibility\n",
        "\n",
        "This creates a cinematic-quality replay of the trained agent.\n",
        "\n"
      ],
      "metadata": {
        "id": "9jMm37sxRBAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.colors as mcolors\n",
        "import numpy as np\n",
        "\n",
        "def render_3d_grid_sequence(grid_sequence, agent_positions, save_path=\"agent_3d_path.mp4\", fps=5):\n",
        "    \"\"\"\n",
        "    Render a sequence of GridWorld frames in true 3D scale, with proportions:\n",
        "    - Each grid tile: 3x3 units\n",
        "    - Agent cube: 3x3x3\n",
        "    - Red obstacle height = 3\n",
        "    - Yellow = 1.5\n",
        "    - Blue = 1.0\n",
        "    - Transparent obstacles\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        grid = grid_sequence[frame]\n",
        "        agent_x, agent_y = agent_positions[frame]\n",
        "\n",
        "        n = grid.shape[0]\n",
        "        cell_unit = 3.0  # Each grid cell is 3x3 in world space\n",
        "\n",
        "        height_map = np.zeros((n, n), dtype=float)\n",
        "        color_map = np.empty((n, n), dtype=object)\n",
        "        alpha_map = np.ones((n, n), dtype=float)\n",
        "\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                val = grid[i, j]\n",
        "                if val == 1:  # Blue\n",
        "                    height_map[i, j] = 1.0\n",
        "                    color_map[i, j] = 'blue'\n",
        "                    alpha_map[i, j] = 0.5\n",
        "                elif val == 2:  # Yellow\n",
        "                    height_map[i, j] = 1.5\n",
        "                    color_map[i, j] = 'yellow'\n",
        "                    alpha_map[i, j] = 0.5\n",
        "                elif val == 3:  # Red\n",
        "                    height_map[i, j] = 3.0\n",
        "                    color_map[i, j] = 'red'\n",
        "                    alpha_map[i, j] = 0.5\n",
        "                elif val == 9:  # Goal\n",
        "                    height_map[i, j] = 0.01\n",
        "                    color_map[i, j] = 'black'\n",
        "                    alpha_map[i, j] = 1.0\n",
        "                else:\n",
        "                    height_map[i, j] = 0.01\n",
        "                    color_map[i, j] = 'lightgray'\n",
        "                    alpha_map[i, j] = 1.0\n",
        "\n",
        "        # Agent cube\n",
        "        height_map[agent_x, agent_y] = 3.0\n",
        "        color_map[agent_x, agent_y] = 'green'\n",
        "        alpha_map[agent_x, agent_y] = 1.0\n",
        "\n",
        "        _x = np.arange(n)\n",
        "        _y = np.arange(n)\n",
        "        _xx, _yy = np.meshgrid(_x, _y)\n",
        "        x, y = _xx.ravel(), _yy.ravel()\n",
        "        z = np.zeros_like(x)\n",
        "\n",
        "        dx = dy = cell_unit\n",
        "        dz = height_map.ravel()\n",
        "        colors = [color_map[i, j] for i in range(n) for j in range(n)]\n",
        "        alphas = alpha_map.ravel()\n",
        "\n",
        "        rgba_colors = []\n",
        "        for c, a in zip(colors, alphas):\n",
        "            rgb = mcolors.to_rgb(c)\n",
        "            rgba_colors.append((*rgb, a))\n",
        "\n",
        "        ax.bar3d(x * dx, y * dy, z, dx, dy, dz, color=rgba_colors, shade=True)\n",
        "\n",
        "        ax.set_xlim(0, n * dx)\n",
        "        ax.set_ylim(0, n * dy)\n",
        "        ax.set_zlim(0, 3.2)\n",
        "        ax.view_init(elev=30, azim=45)\n",
        "        ax.set_box_aspect([1, 1, 0.3])\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f\"Frame {frame + 1}\")\n",
        "\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(grid_sequence), interval=1000 // fps)\n",
        "    ani.save(save_path)\n",
        "    plt.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "L4leToO1VNXr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect frames from 10 episodes\n",
        "grid_frames_all = []\n",
        "agent_positions_all = []\n",
        "\n",
        "num_episodes = 10\n",
        "\n",
        "env = GridWorldEnv(grid_size=15, max_steps=100)\n",
        "\n",
        "for ep in range(num_episodes):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    grid_frames = []\n",
        "    agent_path = []\n",
        "\n",
        "    while not done:\n",
        "        x, y = env.agent_pos\n",
        "        action = np.argmax(q_table[x, y])\n",
        "        grid_frames.append(env.grid.copy())\n",
        "        agent_path.append((x, y))\n",
        "        _, _, done, _, _ = env.step(action)\n",
        "\n",
        "    # Append final position\n",
        "    grid_frames.append(env.grid.copy())\n",
        "    agent_path.append(env.agent_pos)\n",
        "\n",
        "    grid_frames_all.extend(grid_frames)\n",
        "    agent_positions_all.extend(agent_path)\n",
        "\n",
        "# Save the video with full 3D proportions\n",
        "render_3d_grid_sequence(\n",
        "    grid_sequence=grid_frames_all,\n",
        "    agent_positions=agent_positions_all,\n",
        "    save_path=\"agent_3d_path_10episodes.mp4\",\n",
        "    fps=5\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Saved 3D video of 10 episodes as 'agent_3d_path_10episodes.mp4'\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_JmAb8YVR3B",
        "outputId": "c5f3a265-ca46-4b00-a594-2a4812f40a6c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Saved 3D video of 10 episodes as 'agent_3d_path_10episodes.mp4'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Summary: What We Built\n",
        "\n",
        "This notebook creates a fully custom 15x15 GridWorld environment using Gymnasium and Pygame in Google Colab. It includes:\n",
        "\n",
        "- üß† A Q-learning agent that learns to navigate from top to bottom\n",
        "- üî• Reward shaping and obstacle-based penalties\n",
        "- üß± Dynamic obstacles with varying severity (blue/yellow/red)\n",
        "- üß≠ 8-directional movement with automatic rerouting\n",
        "- üé• Visual rendering of test episodes as downloadable `.mp4` video\n",
        "\n",
        "This environment can be extended with:\n",
        "- Moving goals or agents\n",
        "- Dynamic rewards\n",
        "- Curriculum learning with progressive difficulty\n"
      ],
      "metadata": {
        "id": "y2aUiHbaRJa9"
      }
    }
  ]
}